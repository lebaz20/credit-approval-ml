{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=blue>Credit Approval</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=4>1-Needed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import sklearn.metrics as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from itertools import chain, combinations\n",
    "# pip install datawig\n",
    "import datawig\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=4>2- Load Data</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initial state:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.25</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00202</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.460</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>3.04</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>6</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00043</td>\n",
       "      <td>560</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>24.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>1.50</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00280</td>\n",
       "      <td>824</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>27.83</td>\n",
       "      <td>1.540</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>3.75</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00100</td>\n",
       "      <td>3</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>20.17</td>\n",
       "      <td>5.625</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.71</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>00120</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  0      1      2  3  4  5  6     7  8  9   10 11 12     13   14 15\n",
       "0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  f  g  00202    0  +\n",
       "1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  f  g  00043  560  +\n",
       "2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  f  g  00280  824  +\n",
       "3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  t  g  00100    3  +\n",
       "4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  f  s  00120    0  +"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## In case of using Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# file = '/content/drive/MyDrive/Colab Notebooks/crx.data.csv'\n",
    "file = os.path.abspath(\"data/crx.data.csv\")\n",
    "dataset = pd.read_csv(file, header=None)\n",
    "print('Dataset initial state:')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=4>3- Analyze Data</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing data columns:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labib/anaconda3/lib/python3.8/site-packages/pandas/core/ops/__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sex': 'categorical',\n",
       " 'age': 'continuous',\n",
       " 'marital_status': 'categorical',\n",
       " 'customer_type': 'categorical',\n",
       " 'edu_level': 'categorical',\n",
       " 'race': 'categorical',\n",
       " 'balance': 'continuous'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Humanize data by giving variables working names based on the type of data.\n",
    "dataset.columns = [\"sex\", \"age\", \"debt\", \"marital_status\", \"customer_type\", \"edu_level\", \n",
    "                   \"race\", \"years_employed\", \"prior_default\", \"employed\", \"credit_score\",\n",
    "                   \"driver_license\", \"citizen\", \"balance\", \"income\", \"is_approved\"]\n",
    "\n",
    "# Analyze data types based on type of data, as its not always correctly recognized.\n",
    "floatType = 'float'\n",
    "intType = 'int'\n",
    "nonObjectDataTypes = {\n",
    "   \"age\": floatType,\n",
    "   \"debt\": floatType,\n",
    "   \"years_employed\": floatType,\n",
    "   \"credit_score\": intType,\n",
    "   \"balance\": intType,\n",
    "   \"income\": intType,\n",
    "}\n",
    "\n",
    "# Make data more suitable for learning by converting labels to 0,1\n",
    "dataset['is_approved'] = LabelEncoder().fit_transform(dataset['is_approved'])\n",
    "\n",
    "# Gather missing data columns and their data types\n",
    "missingDataColumns = {}\n",
    "continuousType = 'continuous'\n",
    "categoricalType = 'categorical'\n",
    "for column in dataset.columns:\n",
    "    if (dataset[column] == '?').any():\n",
    "        if column in nonObjectDataTypes:\n",
    "            dataType = continuousType\n",
    "        else:\n",
    "            dataType = categoricalType\n",
    "        missingDataColumns[column] = dataType\n",
    "        # replace missing value with NaN\n",
    "        dataset[column] = dataset[column].replace('?', np.nan)\n",
    "\n",
    "# all variations will be collected here, beside saving in csv files\n",
    "datasets = {\n",
    "    \"original\": dataset\n",
    "}\n",
    "\n",
    "print('\\nMissing data columns:')\n",
    "missingDataColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=4>Missing data feasible handling options -there are non-feasible ones as well-</font>\n",
    "    \n",
    "<font color=grey size=3>a- Remove rows with missing data.</font>\n",
    "\n",
    "<font color=grey size=3>b- Fill continuous data with columns mean/median values and categorical data with columns most frequent category.</font> \n",
    "\n",
    "<font color=grey size=3>c- Fill based on predictive models between mostly correlated columns.</font> \n",
    "\n",
    "<font color=grey size=3>d- Fill using unsupervised learning.</font> \n",
    "\n",
    "<font color=grey size=3>e- Fill using deep learning.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=4>4- Remove rows with missing data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Continuous columns info:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>debt</th>\n",
       "      <th>years_employed</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "      <th>is_approved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>653.000000</td>\n",
       "      <td>653.000000</td>\n",
       "      <td>653.000000</td>\n",
       "      <td>653.000000</td>\n",
       "      <td>653.000000</td>\n",
       "      <td>653.000000</td>\n",
       "      <td>653.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31.503813</td>\n",
       "      <td>4.829533</td>\n",
       "      <td>2.244296</td>\n",
       "      <td>2.502297</td>\n",
       "      <td>180.359877</td>\n",
       "      <td>1013.761103</td>\n",
       "      <td>0.546708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.838267</td>\n",
       "      <td>5.027077</td>\n",
       "      <td>3.371120</td>\n",
       "      <td>4.968497</td>\n",
       "      <td>168.296811</td>\n",
       "      <td>5253.278504</td>\n",
       "      <td>0.498195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.580000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.420000</td>\n",
       "      <td>2.835000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>38.250000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>272.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>76.750000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age        debt  years_employed  credit_score      balance  \\\n",
       "count  653.000000  653.000000      653.000000    653.000000   653.000000   \n",
       "mean    31.503813    4.829533        2.244296      2.502297   180.359877   \n",
       "std     11.838267    5.027077        3.371120      4.968497   168.296811   \n",
       "min     13.750000    0.000000        0.000000      0.000000     0.000000   \n",
       "25%     22.580000    1.040000        0.165000      0.000000    73.000000   \n",
       "50%     28.420000    2.835000        1.000000      0.000000   160.000000   \n",
       "75%     38.250000    7.500000        2.625000      3.000000   272.000000   \n",
       "max     76.750000   28.000000       28.500000     67.000000  2000.000000   \n",
       "\n",
       "              income  is_approved  \n",
       "count     653.000000   653.000000  \n",
       "mean     1013.761103     0.546708  \n",
       "std      5253.278504     0.498195  \n",
       "min         0.000000     0.000000  \n",
       "25%         0.000000     0.000000  \n",
       "50%         5.000000     1.000000  \n",
       "75%       400.000000     1.000000  \n",
       "max    100000.000000     1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a. Remove rows with missing data\n",
    "# Pros:\n",
    "#  A model trained with the removal of all missing values creates a robust model.\n",
    "# Cons:\n",
    "#  Loss of a lot of information.\n",
    "#  Works poorly if the percentage of missing values is excessive in comparison to the complete dataset.\n",
    "datasetWithRemovedRows = dataset.copy(deep=True)\n",
    "# remove rows where any column has value NaN\n",
    "datasetWithRemovedRows = datasetWithRemovedRows.dropna()\n",
    "# set data types correctly after removing rows with NaN\n",
    "for column in nonObjectDataTypes:\n",
    "    datasetWithRemovedRows[column] = datasetWithRemovedRows[column].astype(nonObjectDataTypes[column])\n",
    "# save data to a new csv\n",
    "datasetWithRemovedRows.to_csv(\"data/crx.data_removed_missing.csv\", index=False, encoding='utf8')\n",
    "datasets['after removing missing rows'] = datasetWithRemovedRows\n",
    "# validate rows removal is done correctly\n",
    "assert datasetWithRemovedRows.isnull().any().any() == False\n",
    "assert dataset.shape[0] > datasetWithRemovedRows.shape[0]\n",
    "\n",
    "print('\\nContinuous columns info:')\n",
    "datasetWithRemovedRows.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=4>5- Fill continuous data with columns mean values and categorical data with columns most frequent category</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Continuous columns correlation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row0_col0 {\n",
       "            background-color:  #b40426;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row0_col1 {\n",
       "            background-color:  #b2ccfb;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row0_col2 {\n",
       "            background-color:  #ecd3c5;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row0_col3 {\n",
       "            background-color:  #cad8ef;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row0_col4 {\n",
       "            background-color:  #5b7ae5;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row0_col5 {\n",
       "            background-color:  #7295f4;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row0_col6 {\n",
       "            background-color:  #6e90f2;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row1_col0 {\n",
       "            background-color:  #abc8fd;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row1_col1 {\n",
       "            background-color:  #b40426;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row1_col2 {\n",
       "            background-color:  #d6dce4;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row1_col3 {\n",
       "            background-color:  #d8dce2;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row1_col4 {\n",
       "            background-color:  #3b4cc0;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row1_col5 {\n",
       "            background-color:  #8caffe;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row1_col6 {\n",
       "            background-color:  #6788ee;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row2_col0 {\n",
       "            background-color:  #dedcdb;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row2_col1 {\n",
       "            background-color:  #c7d7f0;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row2_col2 {\n",
       "            background-color:  #b40426;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row2_col3 {\n",
       "            background-color:  #e2dad5;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row2_col4 {\n",
       "            background-color:  #6282ea;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row2_col5 {\n",
       "            background-color:  #799cf8;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row2_col6 {\n",
       "            background-color:  #4a63d3;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row3_col0 {\n",
       "            background-color:  #a6c4fe;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row3_col1 {\n",
       "            background-color:  #c0d4f5;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row3_col2 {\n",
       "            background-color:  #dbdcde;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row3_col3 {\n",
       "            background-color:  #b40426;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row3_col4 {\n",
       "            background-color:  #5470de;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row3_col5 {\n",
       "            background-color:  #7a9df8;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row3_col6 {\n",
       "            background-color:  #3b4cc0;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row4_col0 {\n",
       "            background-color:  #536edd;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row4_col1 {\n",
       "            background-color:  #3b4cc0;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row4_col2 {\n",
       "            background-color:  #7b9ff9;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row4_col3 {\n",
       "            background-color:  #7da0f9;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row4_col4 {\n",
       "            background-color:  #b40426;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row4_col5 {\n",
       "            background-color:  #7ea1fa;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row4_col6 {\n",
       "            background-color:  #afcafc;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row5_col0 {\n",
       "            background-color:  #7396f5;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row5_col1 {\n",
       "            background-color:  #96b7ff;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row5_col2 {\n",
       "            background-color:  #9abbff;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row5_col3 {\n",
       "            background-color:  #a9c6fd;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row5_col4 {\n",
       "            background-color:  #89acfd;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row5_col5 {\n",
       "            background-color:  #b40426;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row5_col6 {\n",
       "            background-color:  #6f92f3;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row6_col0 {\n",
       "            background-color:  #3b4cc0;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row6_col1 {\n",
       "            background-color:  #3d50c3;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row6_col2 {\n",
       "            background-color:  #3b4cc0;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row6_col3 {\n",
       "            background-color:  #3b4cc0;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row6_col4 {\n",
       "            background-color:  #8caffe;\n",
       "            color:  #000000;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row6_col5 {\n",
       "            background-color:  #3b4cc0;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_8c99bf1a_6364_11eb_a962_25cd142d57e8row6_col6 {\n",
       "            background-color:  #b40426;\n",
       "            color:  #f1f1f1;\n",
       "        }</style><table id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >age</th>        <th class=\"col_heading level0 col1\" >debt</th>        <th class=\"col_heading level0 col2\" >years_employed</th>        <th class=\"col_heading level0 col3\" >credit_score</th>        <th class=\"col_heading level0 col4\" >balance</th>        <th class=\"col_heading level0 col5\" >income</th>        <th class=\"col_heading level0 col6\" >is_approved</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8level0_row0\" class=\"row_heading level0 row0\" >age</th>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row0_col1\" class=\"data row0 col1\" >0.22</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row0_col2\" class=\"data row0 col2\" >0.42</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row0_col3\" class=\"data row0 col3\" >0.2</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row0_col4\" class=\"data row0 col4\" >-0.085</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row0_col5\" class=\"data row0 col5\" >0.029</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row0_col6\" class=\"data row0 col6\" >-0.18</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8level0_row1\" class=\"row_heading level0 row1\" >debt</th>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row1_col0\" class=\"data row1 col0\" >0.22</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row1_col2\" class=\"data row1 col2\" >0.3</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row1_col3\" class=\"data row1 col3\" >0.27</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row1_col4\" class=\"data row1 col4\" >-0.22</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row1_col5\" class=\"data row1 col5\" >0.12</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row1_col6\" class=\"data row1 col6\" >-0.21</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8level0_row2\" class=\"row_heading level0 row2\" >years_employed</th>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row2_col0\" class=\"data row2 col0\" >0.42</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row2_col1\" class=\"data row2 col1\" >0.3</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row2_col2\" class=\"data row2 col2\" >1</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row2_col3\" class=\"data row2 col3\" >0.33</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row2_col4\" class=\"data row2 col4\" >-0.065</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row2_col5\" class=\"data row2 col5\" >0.052</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row2_col6\" class=\"data row2 col6\" >-0.33</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8level0_row3\" class=\"row_heading level0 row3\" >credit_score</th>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row3_col0\" class=\"data row3 col0\" >0.2</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row3_col1\" class=\"data row3 col1\" >0.27</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row3_col2\" class=\"data row3 col2\" >0.33</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row3_col3\" class=\"data row3 col3\" >1</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row3_col4\" class=\"data row3 col4\" >-0.12</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row3_col5\" class=\"data row3 col5\" >0.058</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row3_col6\" class=\"data row3 col6\" >-0.41</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8level0_row4\" class=\"row_heading level0 row4\" >balance</th>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row4_col0\" class=\"data row4 col0\" >-0.085</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row4_col1\" class=\"data row4 col1\" >-0.22</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row4_col2\" class=\"data row4 col2\" >-0.065</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row4_col3\" class=\"data row4 col3\" >-0.12</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row4_col4\" class=\"data row4 col4\" >1</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row4_col5\" class=\"data row4 col5\" >0.073</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row4_col6\" class=\"data row4 col6\" >0.085</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8level0_row5\" class=\"row_heading level0 row5\" >income</th>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row5_col0\" class=\"data row5 col0\" >0.029</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row5_col1\" class=\"data row5 col1\" >0.12</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row5_col2\" class=\"data row5 col2\" >0.052</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row5_col3\" class=\"data row5 col3\" >0.058</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row5_col4\" class=\"data row5 col4\" >0.073</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row5_col5\" class=\"data row5 col5\" >1</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row5_col6\" class=\"data row5 col6\" >-0.17</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8level0_row6\" class=\"row_heading level0 row6\" >is_approved</th>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row6_col0\" class=\"data row6 col0\" >-0.18</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row6_col1\" class=\"data row6 col1\" >-0.21</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row6_col2\" class=\"data row6 col2\" >-0.33</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row6_col3\" class=\"data row6 col3\" >-0.41</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row6_col4\" class=\"data row6 col4\" >0.085</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row6_col5\" class=\"data row6 col5\" >-0.17</td>\n",
       "                        <td id=\"T_8c99bf1a_6364_11eb_a962_25cd142d57e8row6_col6\" class=\"data row6 col6\" >1</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f31f80812e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b. Fill continuous data with columns mean values and categorical data with columns most frequent category\n",
    "# Pros:\n",
    "#  Prevent data loss which results in deletion of rows or columns.\n",
    "#  Works well with a small dataset and easy to implement.\n",
    "# Cons:\n",
    "#  Can cause data leakage.\n",
    "#  Does not factor the covariance between features (for continuous ones).\n",
    "datasetWithCalculatedRows = dataset.copy(deep=True)\n",
    "for column in missingDataColumns:\n",
    "    if missingDataColumns[column] == continuousType:\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "        imp.fit(datasetWithRemovedRows[column].values.reshape(-1, 1))\n",
    "        datasetWithCalculatedRows[column] = imp.transform(datasetWithCalculatedRows[column].values.reshape(-1, 1))\n",
    "    else:\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "        imp.fit(datasetWithRemovedRows[column].values.reshape(-1, 1))\n",
    "        datasetWithCalculatedRows[column] = imp.transform(datasetWithCalculatedRows[column].values.reshape(-1, 1))\n",
    "# set data types correctly after calculating rows with NaN\n",
    "for column in nonObjectDataTypes:\n",
    "    datasetWithCalculatedRows[column] = datasetWithCalculatedRows[column].astype(nonObjectDataTypes[column])\n",
    "# save data to a new csv\n",
    "datasetWithCalculatedRows.to_csv(\"data/crx.data_calculated_missing.csv\", index=False, encoding='utf8')\n",
    "datasets['after calculating missing rows'] = datasetWithCalculatedRows\n",
    "# validate rows calculation is done correctly\n",
    "assert datasetWithCalculatedRows.isnull().any().any() == False\n",
    "assert dataset.shape[0] == datasetWithCalculatedRows.shape[0]\n",
    "\n",
    "print('\\nContinuous columns correlation:')\n",
    "corr = datasetWithRemovedRows.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=4>6- Fill based on predictive models between mostly correlated columns</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Fill based on predictive models between mostly correlated columns\n",
    "# Pros:\n",
    "#  Gives a better result than earlier methods.\n",
    "#  Takes into account the covariance between missing value column and other columns.\n",
    "# Cons:\n",
    "#  Considered only as a proxy for the true values.\n",
    "datasetWithPredictedRows = dataset.copy(deep=True)\n",
    "# Fill continuous-data-type columns based on predictive models between mostly correlated columns\n",
    "continuousDataColumnsWithoutMissingData = np.append(np.setdiff1d(list(nonObjectDataTypes.keys()),list(missingDataColumns.keys())), 'is_approved')\n",
    "continuousDataColumnsWithMissingData = np.intersect1d(list(nonObjectDataTypes.keys()), list(missingDataColumns.keys()))\n",
    "for column in continuousDataColumnsWithMissingData:\n",
    "    continuousDataColumnsWithoutMissingData = np.append(continuousDataColumnsWithoutMissingData,column)\n",
    "    continuousDatasetWithRemovedRows = datasetWithRemovedRows[continuousDataColumnsWithoutMissingData]\n",
    "    continuousDatasetWithPredictedRows = datasetWithPredictedRows[continuousDataColumnsWithoutMissingData]\n",
    "\n",
    "    xTrain = continuousDatasetWithRemovedRows.drop(column, axis=1)\n",
    "    yTrain = continuousDatasetWithRemovedRows[column]\n",
    "\n",
    "    testData = continuousDatasetWithPredictedRows[continuousDatasetWithPredictedRows[column].isnull()]\n",
    "    xTest = testData.drop(column, axis=1)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(xTrain, yTrain)\n",
    "    yPred = model.predict(xTest)\n",
    "    datasetWithPredictedRows[column][datasetWithPredictedRows[column].isnull()] = np.around(yPred, 2)\n",
    "\n",
    "# Fill nominal-categoricies-data-type columns\n",
    "# Apriori algorithm is a straight-forward Association rule mining technique\n",
    "# to identify underlying relations between different items.\n",
    "#\n",
    "# Support in Apriori is the Fraction of transactions that contain an itemset.\n",
    "# So, the support of item I is defined as the number of transactions containing I divided by the total number of transactions.\n",
    "#\n",
    "# Confidence in Apriori is how often items in Y appear in transactions that contain X.\n",
    "# So, the confidence It’s calculated as the number of transactions containing X and Y divided by the number of transactions containing X.\n",
    "#\n",
    "# Frequent Item Set in Apriori is simply all the itemsets that the support satisfies the minimum support threshold.\n",
    "#\n",
    "# It’s a bottom-up approach. We started from every single item in the itemset list.\n",
    "# Then, the candidates are generated by self-joining.\n",
    "# We extend the length of the itemsets one item at a time.\n",
    "# The subset test is performed at each stage and the itemsets that contain infrequent subsets are pruned.\n",
    "# We repeat the process until no more successful itemsets can be derived from the data.\n",
    "def getAboveMinSup(itemSet, itemSetList, minSup, globalItemSetWithSup):\n",
    "    freqItemSet = set()\n",
    "    localItemSetWithSup = defaultdict(int)\n",
    "\n",
    "    for item in itemSet:\n",
    "        for itemSet in itemSetList:\n",
    "            if item.issubset(itemSet):\n",
    "                globalItemSetWithSup[item] += 1\n",
    "                localItemSetWithSup[item] += 1\n",
    "\n",
    "    for item, supCount in localItemSetWithSup.items():\n",
    "        support = float(supCount / len(itemSetList))\n",
    "        if(support >= minSup):\n",
    "            freqItemSet.add(item)\n",
    "\n",
    "    return freqItemSet\n",
    "\n",
    "\n",
    "def getUnion(itemSet, length):\n",
    "    return set([i.union(j) for i in itemSet for j in itemSet if len(i.union(j)) == length])\n",
    "\n",
    "\n",
    "def pruning(candidateSet, prevFreqSet, length):\n",
    "    tempCandidateSet = candidateSet.copy()\n",
    "    for item in candidateSet:\n",
    "        subsets = combinations(item, length)\n",
    "        for subset in subsets:\n",
    "            # if the subset is not in previous K-frequent get, then remove the set\n",
    "            if(frozenset(subset) not in prevFreqSet):\n",
    "                tempCandidateSet.remove(item)\n",
    "                break\n",
    "    return tempCandidateSet\n",
    "\n",
    "def powerset(s):\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(1, len(s)))\n",
    "\n",
    "def associationRule(freqItemSet, itemSetWithSup, minConf):\n",
    "    rules = []\n",
    "    for k, itemSet in freqItemSet.items():\n",
    "        for item in itemSet:\n",
    "            subsets = powerset(item)\n",
    "            for s in subsets:\n",
    "                confidence = float(\n",
    "                    itemSetWithSup[item] / itemSetWithSup[frozenset(s)])\n",
    "                if(confidence > minConf):\n",
    "                    rules.append([set(s), set(item.difference(s)), confidence])\n",
    "    return rules\n",
    "\n",
    "\n",
    "def getItemSetFromList(itemSetList):\n",
    "    tempItemSet = set()\n",
    "\n",
    "    for itemSet in itemSetList:\n",
    "        for item in itemSet:\n",
    "            tempItemSet.add(frozenset([item]))\n",
    "\n",
    "    return tempItemSet\n",
    "\n",
    "def apriori(itemSetList, minSup, minConf):\n",
    "    C1ItemSet = getItemSetFromList(itemSetList)\n",
    "    # Final result global frequent itemset\n",
    "    globalFreqItemSet = dict()\n",
    "    # Storing global itemset with support count\n",
    "    globalItemSetWithSup = defaultdict(int)\n",
    "\n",
    "    L1ItemSet = getAboveMinSup(\n",
    "        C1ItemSet, itemSetList, minSup, globalItemSetWithSup)\n",
    "    currentLSet = L1ItemSet\n",
    "    k = 2\n",
    "\n",
    "    # Calculating frequent item set\n",
    "    while(currentLSet):\n",
    "        # Storing frequent itemset\n",
    "        globalFreqItemSet[k-1] = currentLSet\n",
    "        # Self-joining Lk\n",
    "        candidateSet = getUnion(currentLSet, k)\n",
    "        # Perform subset testing and remove pruned supersets\n",
    "        candidateSet = pruning(candidateSet, currentLSet, k-1)\n",
    "        # Scanning itemSet for counting support\n",
    "        currentLSet = getAboveMinSup(\n",
    "            candidateSet, itemSetList, minSup, globalItemSetWithSup)\n",
    "        k += 1\n",
    "\n",
    "    rules = associationRule(globalFreqItemSet, globalItemSetWithSup, minConf)\n",
    "    rules.sort(key=lambda x: x[2]*-1)\n",
    "\n",
    "    return globalFreqItemSet, rules\n",
    "\n",
    "categoricalDataColumnsWithoutMissingData = np.setdiff1d(np.setdiff1d(dataset.columns,list(nonObjectDataTypes.keys())),list(missingDataColumns.keys()))\n",
    "categoricalDataColumnsWithMissingData = np.setdiff1d(list(missingDataColumns.keys()), list(nonObjectDataTypes.keys()))\n",
    "\n",
    "\n",
    "for missingDataColumn in categoricalDataColumnsWithMissingData:\n",
    "    categoricalDataColumnsWithoutMissingData = np.append(categoricalDataColumnsWithoutMissingData,missingDataColumn)\n",
    "    categoricalDatasetWithRemovedRows = datasetWithRemovedRows[categoricalDataColumnsWithoutMissingData]\n",
    "    categoricalDatasetWithPredictedRows = datasetWithPredictedRows[categoricalDataColumnsWithoutMissingData]\n",
    "    # Make each column unique even if there are common values between columns like `t` and `f`\n",
    "    for column in categoricalDatasetWithRemovedRows:\n",
    "        categoricalDatasetWithRemovedRows[column] = [column + '_' + str(rowValue) for rowValue in categoricalDatasetWithRemovedRows[column]]\n",
    "    # Use apriori to collect association rules\n",
    "    globalFreqItemSet, rules = apriori(categoricalDatasetWithRemovedRows.to_numpy(), 0.5, 0.5)\n",
    "    usefulRules = []\n",
    "    columnUniqueValues = categoricalDatasetWithRemovedRows[column].unique()\n",
    "    for rule in rules:\n",
    "        ruleValues = list(rule[0]) + list(rule[1])\n",
    "        for columnValue in columnUniqueValues:\n",
    "            # keep rules that include the column with missing data\n",
    "            if columnValue in ruleValues:\n",
    "                usefulRules.append(ruleValues)\n",
    "    # Rename prediction data similarly to training data to be able to match \n",
    "    for column in categoricalDatasetWithPredictedRows[categoricalDatasetWithPredictedRows[column].isnull()]:\n",
    "        if missingDataColumn != column:\n",
    "            categoricalDatasetWithPredictedRows[column] = [column + '_' + str(rowValue) for rowValue in categoricalDatasetWithPredictedRows[column]]\n",
    "    # Predict missing data\n",
    "    # Keep in mind, some rows might still have no clear association rules to predict.\n",
    "    # We will use most frequent value for those.\n",
    "    yPred = [];\n",
    "    stillHaveMissingValues = False\n",
    "    for row in categoricalDatasetWithPredictedRows[categoricalDatasetWithPredictedRows[column].isnull()].drop(column, axis=1).to_numpy():\n",
    "        r = len(row)\n",
    "        matched = False\n",
    "        # collect all possible combinations of row starting from the whole row to single column values.\n",
    "        # first match with association rules is used to predict missing column value.\n",
    "        while(r >= 1 and matched == False):\n",
    "            for combination in combinations(row, r):\n",
    "                if matched == False:\n",
    "                    for rule in usefulRules:\n",
    "                        if matched == False and len(np.intersect1d(combination,rule)) == len(combination):\n",
    "                            # get missing column value from association rule\n",
    "                            matching = [s for s in rule if missingDataColumn in s]\n",
    "                            # remove added unique string from matched value\n",
    "                            yPred.append(matching[0].split('_')[-1])\n",
    "                            # stop looking for matched values for that row\n",
    "                            matched = True\n",
    "            r -= 1\n",
    "        # if no match found, keep value as `NaN`\n",
    "        if matched == False:\n",
    "            stillHaveMissingValues = True\n",
    "            yPred.append(np.nan)\n",
    "    datasetWithPredictedRows[missingDataColumn][datasetWithPredictedRows[missingDataColumn].isnull()] = yPred\n",
    "    if stillHaveMissingValues == True:\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "        imp.fit(datasetWithPredictedRows[missingDataColumn].values.reshape(-1, 1))\n",
    "        datasetWithPredictedRows[missingDataColumn] = imp.transform(datasetWithPredictedRows[missingDataColumn].values.reshape(-1, 1))\n",
    "# set data types correctly after predicting rows with NaN\n",
    "for column in nonObjectDataTypes:\n",
    "    datasetWithPredictedRows[column] = datasetWithPredictedRows[column].astype(nonObjectDataTypes[column])\n",
    "# save data to a new csv\n",
    "datasetWithPredictedRows.to_csv(\"data/crx.data_predicted_missing.csv\", index=False, encoding='utf8')\n",
    "datasets['after predicting missing rows'] = datasetWithPredictedRows\n",
    "# validate rows prediction is done correctly\n",
    "assert datasetWithPredictedRows.isnull().any().any() == False\n",
    "assert dataset.shape[0] == datasetWithPredictedRows.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=4>7- Fill using unsupervised learning.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Fill using unsupervised learning\n",
    "# Pros:\n",
    "# Support more non-linearity between data, as it doesn't need strong correlation  \n",
    "#  Takes into account the covariance between missing value column and other columns.\n",
    "# Cons:\n",
    "#  Considered only as a proxy for the true values.\n",
    "datasetWithUnsupervisedLearntRows = dataset.copy(deep=True)\n",
    "\n",
    "continuousDataColumns = list(nonObjectDataTypes.keys())\n",
    "categoricalDataColumns = np.setdiff1d(dataset.columns,continuousDataColumns)\n",
    "\n",
    "categoriesMap = {}\n",
    "# Convert categorical columns data to ordinal integers\n",
    "for column in categoricalDataColumns:\n",
    "    columnUniqueValues = datasetWithRemovedRows[column].unique()\n",
    "    categoryMap = ({ index: v for index, v in enumerate(columnUniqueValues) })\n",
    "    keyList = list(categoryMap.keys())\n",
    "    valList = list(categoryMap.values())\n",
    "    categoriesMap[column] = {'keys': keyList, 'values': valList, 'ordinalMap': categoryMap}\n",
    "    datasetWithUnsupervisedLearntRows[column][datasetWithUnsupervisedLearntRows[column] == datasetWithUnsupervisedLearntRows[column]] = [keyList[valList.index(rowValue)] for rowValue in datasetWithUnsupervisedLearntRows[column] if rowValue == rowValue]\n",
    "    \n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "transformedData = imputer.fit_transform(datasetWithUnsupervisedLearntRows)\n",
    "rotatedTransformedData = transformedData.T\n",
    "for index, column in enumerate(datasetWithUnsupervisedLearntRows.columns):\n",
    "    datasetWithUnsupervisedLearntRows[column] = rotatedTransformedData[index]\n",
    "\n",
    "# Convert ordinal integers to original categorical columns data\n",
    "for column in categoricalDataColumns:\n",
    "    datasetWithUnsupervisedLearntRows[column] = [categoriesMap[column]['ordinalMap'][round(rowValue,0)] for rowValue in datasetWithUnsupervisedLearntRows[column]]\n",
    "\n",
    "# set data types correctly after unsupervised learning rows with NaN\n",
    "for column in nonObjectDataTypes:\n",
    "    datasetWithUnsupervisedLearntRows[column] = datasetWithUnsupervisedLearntRows[column].astype(nonObjectDataTypes[column])\n",
    "# save data to a new csv\n",
    "datasetWithUnsupervisedLearntRows.to_csv(\"data/crx.data_unsupervised_learnt_missing.csv\", index=False, encoding='utf8')\n",
    "datasets['after unsupervised learning missing rows'] = datasetWithUnsupervisedLearntRows\n",
    "# validate rows unsupervised learning is done correctly\n",
    "assert datasetWithUnsupervisedLearntRows.isnull().any().any() == False\n",
    "assert dataset.shape[0] == datasetWithUnsupervisedLearntRows.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=4>8- Fill using deep learning.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labib/anaconda3/lib/python3.8/site-packages/datawig/calibration.py:92: RuntimeWarning: invalid value encountered in log\n",
      "  return np.log(probas)\n",
      "/home/labib/anaconda3/lib/python3.8/site-packages/datawig/calibration.py:92: RuntimeWarning: invalid value encountered in log\n",
      "  return np.log(probas)\n",
      "/home/labib/anaconda3/lib/python3.8/site-packages/datawig/calibration.py:92: RuntimeWarning: invalid value encountered in log\n",
      "  return np.log(probas)\n",
      "/home/labib/anaconda3/lib/python3.8/site-packages/datawig/calibration.py:92: RuntimeWarning: invalid value encountered in log\n",
      "  return np.log(probas)\n",
      "/home/labib/anaconda3/lib/python3.8/site-packages/datawig/calibration.py:92: RuntimeWarning: invalid value encountered in log\n",
      "  return np.log(probas)\n"
     ]
    }
   ],
   "source": [
    "# e. Fill using deep learning\n",
    "# Pros:\n",
    "#  Quite accurate compared to other methods.\n",
    "#  It supports both CPUs and GPUs.\n",
    "# Cons:\n",
    "#  Still can be quite slow with large datasets.\n",
    "datasetWithDeepLearntRows = dataset.copy(deep=True)\n",
    "\n",
    "continuousDataColumns = list(nonObjectDataTypes.keys())\n",
    "categoricalDataColumns = np.setdiff1d(dataset.columns,continuousDataColumns)\n",
    "\n",
    "categoriesMap = {}\n",
    "# Convert categorical columns data to ordinal integers\n",
    "for column in categoricalDataColumns:\n",
    "    columnUniqueValues = datasetWithRemovedRows[column].unique()\n",
    "    categoryMap = ({ index: v for index, v in enumerate(columnUniqueValues) })\n",
    "    keyList = list(categoryMap.keys())\n",
    "    valList = list(categoryMap.values())\n",
    "    categoriesMap[column] = {'keys': keyList, 'values': valList, 'ordinalMap': categoryMap}\n",
    "    datasetWithDeepLearntRows[column][datasetWithDeepLearntRows[column] == datasetWithDeepLearntRows[column]] = [keyList[valList.index(rowValue)] for rowValue in datasetWithDeepLearntRows[column] if rowValue == rowValue]\n",
    "\n",
    "columns = dataset.columns.values\n",
    "for column in columns:\n",
    "    datasetWithDeepLearntRows[column] = datasetWithDeepLearntRows[column].astype(floatType)\n",
    "\n",
    "\n",
    "for column in missingDataColumns:\n",
    "    df_train = datasetWithDeepLearntRows.dropna()\n",
    "    df_test = datasetWithDeepLearntRows[datasetWithDeepLearntRows[column].isnull()]\n",
    "    # Initialize a SimpleImputer model\n",
    "    imputer = datawig.SimpleImputer(\n",
    "        input_columns=np.delete(columns,np.where(columns == column)), # column(s) containing information about the column we want to impute\n",
    "        output_column= column, # the column we'd like to impute values for\n",
    "        output_path = 'imputer_model' # stores model data and metrics\n",
    "        )\n",
    "\n",
    "    # Fit an imputer model on the train data\n",
    "    imputer.fit(train_df=df_train, num_epochs=50)\n",
    "\n",
    "    # Impute missing values and return original dataframe with predictions\n",
    "    imputed = imputer.predict(df_test)\n",
    "    datasetWithDeepLearntRows[column][datasetWithDeepLearntRows[column].isnull()] = np.around(imputed[column+'_imputed'], 2)\n",
    "\n",
    "# Convert ordinal integers to original categorical columns data\n",
    "for column in categoricalDataColumns:\n",
    "    datasetWithDeepLearntRows[column] = [categoriesMap[column]['ordinalMap'][np.abs(round(rowValue,0))] for rowValue in datasetWithDeepLearntRows[column]]\n",
    "\n",
    "# set data types correctly after deep learning rows with NaN\n",
    "for column in nonObjectDataTypes:\n",
    "    datasetWithDeepLearntRows[column] = datasetWithDeepLearntRows[column].astype(nonObjectDataTypes[column])\n",
    "# save data to a new csv\n",
    "datasetWithDeepLearntRows.to_csv(\"data/crx.data_deep_learnt_missing.csv\", index=False, encoding='utf8')\n",
    "datasets['after deep learning missing rows'] = datasetWithDeepLearntRows\n",
    "# validate rows deep learning is done correctly\n",
    "assert datasetWithDeepLearntRows.isnull().any().any() == False\n",
    "assert dataset.shape[0] == datasetWithDeepLearntRows.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=4>9- Normalize Data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
